# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

Source: hadoop
Section: misc
Priority: extra
Maintainer: Cloudera Inc. <https://issues.cloudera.org>
Build-Depends: debhelper (>= 7.0.50~), liblzo2-dev, libzip-dev, sharutils, g++ (>= 4), libfuse-dev, libssl-dev, cmake, pkg-config, python, automake, autoconf (>= 2.61), make, libsnappy1-dev | libsnappy-dev
Standards-Version: 3.8.0
Homepage: http://hadoop.apache.org/core/

Package: hadoop
Architecture: all
Depends: ${shlibs:Depends}, ${misc:Depends}, adduser, bigtop-utils (>= 0.7), zookeeper (>= 3.4.0), psmisc, netcat-openbsd, avro-libs, parquet
Description: A software platform for processing vast amounts of data
 Hadoop is a software platform that lets one easily write and
 run applications that process vast amounts of data.
 .
 Here's what makes Hadoop especially useful:
  * Scalable: Hadoop can reliably store and process petabytes.
  * Economical: It distributes the data and processing across clusters
                of commonly available computers. These clusters can number
                into the thousands of nodes.
  * Efficient: By distributing the data, Hadoop can process it in parallel
               on the nodes where the data is located. This makes it
               extremely rapid.
  * Reliable: Hadoop automatically maintains multiple copies of data and
              automatically redeploys computing tasks based on failures.
 .
 Hadoop implements MapReduce, using the Hadoop Distributed File System (HDFS).
 MapReduce divides applications into many small blocks of work. HDFS creates
 multiple replicas of data blocks for reliability, placing them on compute
 nodes around the cluster. MapReduce can then process the data where it is
 located.

Package: hadoop-hdfs
Architecture: all
Depends: ${shlibs:Depends}, ${misc:Depends}, adduser, bigtop-utils (>= 0.7), hadoop (= ${source:Version}), bigtop-jsvc
Description: The Hadoop Distributed File System
 Hadoop Distributed File System (HDFS) is the primary storage system used by
 Hadoop applications. HDFS creates multiple replicas of data blocks and
 distributes them on cluster hosts to enable reliable and extremely rapid
 computations.

Package: hadoop-yarn
Architecture: all
Depends: ${shlibs:Depends}, ${misc:Depends}, adduser, bigtop-utils (>= 0.7), hadoop (= ${source:Version}), avro-libs, zookeeper
Description: The Hadoop NextGen MapReduce (YARN)
 YARN (Hadoop NextGen MapReduce) is a general purpose data-computation framework.
 YARN splits up the functionalities of JobTracker, resource management, 
 job scheduling, and job monitoring into separate daemons called 
 ResourceManager and NodeManager.
 .
 ResourceManager is the ultimate authority that arbitrates resources 
 among all applications in the system. NodeManager is a per-host slave
 that manages allocation of computational resources on a single host. 
 Both daemons work in support of ApplicationMaster (AM).
 .
 ApplicationMaster is a framework-specific library that negotiates resources 
 from ResourceManager and works with NodeManager(s) to execute and monitor 
 the tasks.

Package: hadoop-mapreduce
Architecture: all
Depends: ${shlibs:Depends}, ${misc:Depends}, adduser, bigtop-utils (>= 0.7), hadoop-yarn (= ${source:Version}), avro-libs, zookeeper, libnativetask1 (= ${source:Version})
Description: The Hadoop MapReduce (MRv2)
 Hadoop MapReduce is a programming model and software framework for
 writing applications that rapidly process vast amounts of data
 in parallel on large clusters of hosts.

Package: hadoop-hdfs-fuse
Architecture: i386 amd64
Depends: ${shlibs:Depends}, hadoop-hdfs (= ${source:Version}), hadoop-client (= ${source:Version}), libfuse2, fuse-utils|fuse, bigtop-utils (>= 0.7)
Enhances: hadoop
Description: HDFS exposed over a Filesystem in Userspace
 These projects (enumerated below) allow HDFS to be mounted (on most flavors 
 of Unix) as a standard file system using the mount command. Once mounted, the
  user can operate on an instance of hdfs using standard Unix utilities such 
 as 'ls', 'cd', 'cp', 'mkdir', 'find', 'grep', or use standard Posix libraries 
 like open, write, read, close from C, C++, Python, Ruby, Perl, Java, bash, etc.

Package: hadoop-doc
Architecture: all
Section: doc
Description: Documentation for Hadoop
 This package contains the Java Documentation for Hadoop and its relevant
 APIs.

Package: hadoop-conf-pseudo
Architecture: all
Depends: hadoop (= ${source:Version}), hadoop-hdfs-namenode (= ${source:Version}),
         hadoop-hdfs-datanode (= ${source:Version}), hadoop-hdfs-secondarynamenode (= ${source:Version}),
         hadoop-yarn-resourcemanager (= ${source:Version}), hadoop-yarn-nodemanager (= ${source:Version}),
         hadoop-mapreduce-historyserver (= ${source:Version})
Description: Pseudo-distributed Hadoop configuration
 Contains configuration files for a "pseudo-distributed" Hadoop deployment.
 In this mode, each of the hadoop components runs as a separate Java process,
 but all on the same machine.

Package: hadoop-mapreduce-historyserver
Architecture: all
Depends: hadoop-mapreduce (= ${source:Version}), hadoop-hdfs (= ${source:Version})
Description: MapReduce History Server
 The History server keeps records of the different activities being performed 
 on a Apache Hadoop cluster.

Package: hadoop-yarn-nodemanager
Architecture: all
Depends: hadoop-yarn (= ${source:Version})
Description: Node manager for Hadoop
 The NodeManager is the per-machine framework agent who is responsible for
 containers, monitoring their resource usage (cpu, memory, disk, network) and
 reporting the same to the ResourceManager/Scheduler.

Package: hadoop-yarn-resourcemanager
Architecture: all
Depends: hadoop-yarn (= ${source:Version})
Description: Resource manager for Hadoop
 The resource manager manages the global assignment of compute resources to applications.

Package: hadoop-yarn-proxyserver
Architecture: all
Depends: hadoop-yarn (= ${source:Version})
Description: Web proxy for YARN
 The web proxy server sits in front of the YARN application master web UI.

Package: hadoop-hdfs-namenode
Architecture: all
Depends: hadoop-hdfs (= ${source:Version})
Description: Name Node for Hadoop
 The Hadoop Distributed Filesystem (HDFS) requires one unique server, the
 namenode, which manages the block locations of files on the filesystem.

Package: hadoop-hdfs-secondarynamenode
Architecture: all
Depends: hadoop-hdfs (= ${source:Version})
Description: Secondary Name Node for Hadoop
 The Secondary Name Node is responsible for checkpointing file system images.
 It is _not_ a failover pair for the namenode, and may safely be run on the
 same machine.

Package: hadoop-hdfs-zkfc
Architecture: all
Depends: hadoop-hdfs (= ${source:Version})
Description: Hadoop HDFS failover controller
 The Hadoop HDFS failover controller is a ZooKeeper client which also
 monitors and manages the state of the NameNode. Each of the machines
 which runs a NameNode also runs a ZKFC, and that ZKFC is responsible
 for: Health monitoring, ZooKeeper session management, ZooKeeper-based
 election.

Package: hadoop-hdfs-journalnode
Provides: hadoop-hdfs-journalnode
Architecture: all
Depends: hadoop-hdfs (= ${source:Version})
Description: Hadoop HDFS JournalNode 
 The HDFS JournalNode is responsible for persisting NameNode edit logs. 
 In a typical deployment the JournalNode daemon runs on at least three 
 separate machines in the cluster.

Package: hadoop-hdfs-datanode
Architecture: all
Depends: hadoop-hdfs (= ${source:Version})
Description: Data Node for Hadoop
 The Data Nodes in the Hadoop Cluster are responsible for serving up
 blocks of data over the network to Hadoop Distributed Filesystem
 (HDFS) clients.

Package: hadoop-hdfs-nfs3
Architecture: all
Depends: hadoop-hdfs (= ${source:Version}), rpcbind
Description: Hadoop HDFS NFS v3 gateway service

Package: libnativetask1
Architecture: all
Depends: ${shlibs:Depends}
Description: Native task implementation for MapReduce

Package: libhdfs0
Architecture: any
Depends: hadoop (= ${source:Version}), ${shlibs:Depends}
Description: JNI Bindings to access Hadoop HDFS from C
 See http://wiki.apache.org/hadoop/LibHDFS
 
Package: hadoop-kms
Architecture: all
Depends: hadoop (= ${source:Version}), hadoop-client (= ${source:Version}), bigtop-tomcat, zookeeper
Description: KMS for Hadoop
   Hadoop KMS is a cryptographic Key Management Server based on Hadoop's KeyProvider API.

Package: libhdfs0-dev
Architecture: any
Section: libdevel
Depends: hadoop (= ${source:Version}), libhdfs0 (= ${binary:Version})
Description: Development support for libhdfs0
 Includes examples and header files for accessing HDFS from C

Package: hadoop-httpfs
Architecture: all
Depends: hadoop (= ${source:Version}), hadoop-client (= ${source:Version}), hadoop-hdfs (= ${source:Version}), bigtop-tomcat, avro-libs, zookeeper
Description: HTTPFS for Hadoop
  The server providing HTTP REST API support for the complete FileSystem/FileContext
  interface in HDFS.

Package: hadoop-client
Architecture: all
Replaces: hadoop-0.20-mapreduce (<< 0.20.2+1247)
Depends: hadoop (= ${source:Version}), hadoop-hdfs (= ${source:Version}), zookeeper,
         hadoop-yarn (= ${source:Version}), hadoop-mapreduce (= ${source:Version}), hadoop-0.20-mapreduce (= ${source:Version}), avro-libs
Description: Hadoop client side dependencies
 Installation of this package will provide you with all the dependencies for Hadoop clients.

Package: hadoop-0.20-mapreduce
Architecture: i386 amd64 
Depends: ${shlibs:Depends}, ${misc:Depends}, hadoop (= ${source:Version}), hadoop-hdfs (= ${source:Version}), adduser, avro-libs, zookeeper
Description: A software platform for processing vast amounts of data
 Hadoop is a software platform that lets one easily write and
 run applications that process vast amounts of data.
 .
 Here's what makes Hadoop especially useful:
  * Scalable: Hadoop can reliably store and process petabytes.
  * Economical: It distributes the data and processing across clusters
                of commonly available computers. These clusters can number
                into the thousands of nodes.
  * Efficient: By distributing the data, Hadoop can process it in parallel
               on the nodes where the data is located. This makes it
               extremely rapid.
  * Reliable: Hadoop automatically maintains multiple copies of data and
              automatically redeploys computing tasks based on failures.
 .
 Hadoop implements MapReduce, using the Hadoop Distributed File System (HDFS).
 MapReduce divides applications into many small blocks of work. HDFS creates
 multiple replicas of data blocks for reliability, placing them on compute
 nodes around the cluster. MapReduce can then process the data where it is
 located.

Package: hadoop-0.20-mapreduce-tasktracker
Architecture: all
Depends: hadoop-0.20-mapreduce (= ${source:Version})
Replaces: hadoop-0.20-mapreduce (<< 0.20.2+737-1)
Breaks: hadoop-0.20-mapreduce (<< 0.20.2+737-1)
Description: Task Tracker for Hadoop
 The Task Tracker is the Hadoop service that accepts MapReduce tasks and
 computes results. Each node in a Hadoop cluster that should be doing
 computation should run a Task Tracker.

Package: hadoop-0.20-mapreduce-jobtracker
Architecture: all
Depends: hadoop-0.20-mapreduce (= ${source:Version})
Replaces: hadoop-0.20-mapreduce (<< 0.20.2+737-1)
Breaks: hadoop-0.20-mapreduce (<< 0.20.2+737-1), hadoop-0.20-mapreduce-jobtrackerha
Description: JobTracker for Hadoop
 The jobtracker is a central service which is responsible for managing
 the tasktracker services running on all nodes in a Hadoop Cluster.
 The jobtracker allocates work to the tasktracker nearest to the data
 with an available work slot.

Package: hadoop-0.20-conf-pseudo
Provides: hadoop-0.20-conf-pseudo
Architecture: all
Depends: hadoop (= ${source:Version}), hadoop-hdfs-namenode (= ${source:Version}), hadoop-hdfs-datanode (= ${source:Version}), hadoop-hdfs-secondarynamenode (= ${source:Version}), hadoop-0.20-mapreduce-jobtracker (= ${source:Version}), hadoop-0.20-mapreduce-tasktracker (= ${source:Version})
Breaks: hadoop-conf-pseudo
Description: Hadoop installation in pseudo-distributed mode with MRv1
 Installation of this RPM will setup your machine to run in pseudo-distributed mode
 where each Hadoop daemon runs in a separate Java process. You will be getting old
 style daemons (MRv1) for Hadoop jobtracker and Hadoop tasktracker instead of new
 YARN (MRv2) ones.

Package: hadoop-0.20-mapreduce-jobtrackerha
Provides: hadoop-0.20-mapreduce-jobtrackerha
Architecture: all
Depends: hadoop-0.20-mapreduce (= ${source:Version})
Breaks: hadoop-0.20-mapreduce-jobtracker, hadoop-0.20-jobtracker
Description: High Availability JobTracker for Hadoop
 The Hadoop MapReduce JobTracker High Availability Daemon provides a 
 High Availability JobTracker. JobTracker (installed by 
 hadoop-0.20-mapreduce-jobtracker) and JobTracker High Availability 
 (installed by this package - hadoop-0.20-mapreduce-jobtrackerha)
 can not be installed together on the same machine. Only one of them should
 be installed on a given machine at any given time. When used in coordination
 with Hadoop MapReduce failover controller (installed by
 hadoop-0.20-mapreduce-zkfc), this JobTracker provides automatic failover.
 The jobtracker is a central service which is responsible for managing
 the tasktracker services running on all nodes in a Hadoop Cluster.
 The jobtracker allocates work to the tasktracker nearest to the data
 with an available work slot.

Package: hadoop-0.20-mapreduce-zkfc
Provides: hadoop-0.20-mapreduce-zkfc
Architecture: all
Depends: hadoop-0.20-mapreduce-jobtrackerha (= ${source:Version}), zookeeper (>= 3.4.0)
Description: Hadoop MapReduce failover controller
 The Hadoop MapReduce failover controller is a Zookeeper client which also
 manages the state of the JobTracker. Any machines running ZKFC also need to
 run High Availability JobTracker (installed by hadoop-0.20-mapreduce-jobtrackerha).
 The ZKFC is responsible for: Health monitoring, Zookeeper
 session management and Zookeeper-based election.
